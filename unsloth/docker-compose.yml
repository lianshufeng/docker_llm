services:
  unsloth_build:
    build:
      args:
        http_proxy: http://192.168.31.98:1080
        https_proxy: http://192.168.31.98:1080
      context: ./
      dockerfile: Dockerfile
    image: lianshufeng/llm:unsloth
    # shm_size: 6.15g
    privileged: true
    container_name: unsloth-train
    # restart: always
    environment:
      # - http_proxy=http://192.168.31.98:1080
      # - https_proxy=http://192.168.31.98:1080
      - CUDA_VISIBLE_DEVICES=0 # 多卡 0,1
      
      #- HF_TOKEN=hf_* #token
      - HF_ENDPOINT=https://hf-mirror.com #镜像代理
      
      # 模型
      # - model_name=unsloth/Phi-3-mini-4k-instruct
      # - model_name=Qwen/Qwen1.5-7B-Chat
      # - model_name=YeungNLP/firefly-qwen1.5-en-7b-unsloth
      # - model_name=unsloth/llama-3-8b-Instruct-bnb-4bit
      # - model_name=unsloth/Phi-3.5-mini-instruct
      - model_name=/model
      # 保存模型
      - merged_method=merged_16bit
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: [gpu]
    volumes:
      - ./scripts:/scripts
      - ./store/.cache/huggingface:/root/.cache/huggingface
      - ./store/datasets/hf:/datasets/hf
      - ./store/outputs:/outputs
      # - E:/git/huggingface/Qwen/Qwen1.5-7B-Chat:/model #本地模型
      # - E:/git/huggingface/Qwen/Qwen2-7B-Instruct:/model #本地模型
      # - E:/git/huggingface/Qwen/Qwen2-7B-Instruct:/model #本地模型
      - E:/git/huggingface/Qwen/Qwen2.5-7B-Instruct:/model
      # - E:/git/huggingface/BelleGroup/train_0.5M_CN:/datasets/train #数据集
      - E:\git\huggingface\yunfeng\datasets_min:/datasets/train
      # - E:/git/huggingface/yahma/alpaca-cleaned:/datasets/train
    # command: python3 /scripts/Phi-3.py
    command: python3 /scripts/Qwen-2.py
    # command: python3 /scripts/llama-3.py




# docker-compose build --no-cache
# docker-compose down && docker-compose up -d
