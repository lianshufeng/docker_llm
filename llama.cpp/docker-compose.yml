services:
  llama-server:
    build:
      args:
        http_proxy: http://192.168.31.98:1080
        https_proxy: http://192.168.31.98:1080
      context: ./
      dockerfile: Dockerfile  
    image: lianshufeng/llm:llama-server
    container_name: llama-server
    volumes:
      - ./model.gguf:/model.gguf
    restart: always
    ports:
      - "8080:8080"
    command: -m /model.gguf --port 8080 --host 0.0.0.0 -n 4096
