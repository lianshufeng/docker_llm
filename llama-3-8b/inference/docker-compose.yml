version: "3"

services:

  llama3:
    build:
      args:
        http_proxy: http://192.168.31.98:1080
        https_proxy: http://192.168.31.98:1080
      context: ./
      dockerfile: Dockerfile
    image: lianshufeng/llm:llama-3-inference
    shm_size: 6.15g
    ports:
      - "8000:8000"
    privileged: true
    container_name: llama-3-inference
    restart: unless-stopped
    environment:
      - CUDA_VISIBLE_DEVICES=0,1 # 多卡 0,1
      # - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: [gpu]
    volumes:
      - ../train/store/outputs/merged:/models
      # - E:/git/huggingface/meta-llama/Meta-Llama-3-8B-Instruct:/models
    command: python3 -m vllm.entrypoints.openai.api_server --served-model-name gpt-3.5-turbo --model /models  --gpu-memory-utilization 0.9   --tensor-parallel-size 2 --max-model-len 1000 --dtype half --disable-custom-all-reduce
    
    
# docker-compose down && docker-compose up --build
# api:   "stop":"<|eot_id|>",
# 多卡： --disable-custom-all-reduce && shm_size: 6.15g